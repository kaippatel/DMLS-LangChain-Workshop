{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaippatel/dmls-langchain-workshop/blob/main/RAG_Text_Splitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zmSjM8VHR1r"
      },
      "source": [
        "# **RAG Text Splitting options**\n",
        "\n",
        "\n",
        "## **📌Overview**\n",
        "RAG Text Splitting Overview\n",
        "RAG (Retrieval-Augmented Generation) text splitting involves dividing input text or documents into smaller, coherent segments (chunks) to optimize retrieval of relevant information. This preprocessing step balances context preservation (e.g., using semantic or overlapping splits) with efficient processing, ensuring the retriever fetches meaningful data for the generator to produce accurate, context-aware outputs. There are several strategies to enhance RAG performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pos9TcsRHR1s"
      },
      "outputs": [],
      "source": [
        "# First uninstall conflicting versions\n",
        "!pip uninstall -y google-generativeai google-ai-generativelanguage\n",
        "\n",
        "# Then install all dependencies in one go\n",
        "!pip install -qU \\\n",
        "  python-dotenv \\\n",
        "  langchain-core \\\n",
        "  langchain-google-genai \\\n",
        "  chromadb \\\n",
        "  pypdf \\\n",
        "  langchain-community \\\n",
        "  google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6InDDbjcHR1t"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqEgH1oDHR1t"
      },
      "source": [
        "## **Obtain a Google Gemini API Key (GOOGLE COLLAB SETUP):**\n",
        "\n",
        "If you have a Google Gemini API Key:\n",
        "- Copy your API key and replace \"your_google_api_key_here\" in the code below\n",
        "\n",
        "Otherwise:  \n",
        "- Go to the Google AI Studio API Console: [Google AI Studio](https://aistudio.google.com/prompts/new_chat)\n",
        "- Sign in with your Google account and create a new API key.\n",
        "- Copy your API key and replace \"your_google_api_key_here\" in the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl8cAlP9HR1t"
      },
      "outputs": [],
      "source": [
        "# Set your Google API key manually\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwtTIegkHR1t"
      },
      "source": [
        "## **Load Environment Variables (LOCAL SETUP)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIOa1JCMHR1t"
      },
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1H_TZdFHR1u"
      },
      "source": [
        "---\n",
        "\n",
        "## **Imports**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AQD6sXCHR1u"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mecM0GMqHR1u"
      },
      "source": [
        "## **Colab File Setup**   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jstWgw83HR1u"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload your 1984.txt when prompted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUAXYlq4HR1u"
      },
      "source": [
        "\n",
        "## **Use current directory for persistence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uUdele2HR1u"
      },
      "outputs": [],
      "source": [
        "persistent_directory = \"/content/chroma_db\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URX7e3reHR1v"
      },
      "source": [
        "## **Load Document**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0infmn-HR1v"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"1984.txt\")  # Use uploaded filename\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function to create vector stores**\\"
      ],
      "metadata": {
        "id": "VJ7XaHtnpo2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(docs, store_name):\n",
        "    store_path = os.path.join(persistent_directory, store_name)\n",
        "    if not os.path.exists(store_path):\n",
        "        print(f\"\\n--- Creating vector store: {store_name} ---\")\n",
        "        db = Chroma.from_documents(\n",
        "            docs,\n",
        "            embeddings,\n",
        "            persist_directory=store_path\n",
        "        )\n",
        "        print(f\"--- Finished creating vector store: {store_name} ---\")\n",
        "    else:\n",
        "        print(f\"Vector store '{store_name}' already exists, skipping.\")\n"
      ],
      "metadata": {
        "id": "z55lbQwBpnKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Splitting Options**\n",
        "\n",
        "Character Splitting\n",
        "Divides text into fixed-size chunks (e.g., 500 characters). Simple but risks splitting words/sentences mid-context. Ideal for raw preprocessing.\n",
        "\n",
        "Sentence Splitting\n",
        "Splits text at sentence boundaries (using NLP tools like spaCy or NLTK). Preserves semantic coherence but may create uneven chunk sizes.\n",
        "\n",
        "Token Splitting\n",
        "Splits text into token units (e.g., words/subwords aligned with model tokenizers like GPT). Ensures chunks fit model limits without breaking tokens.\n",
        "\n",
        "Recursive Splitting\n",
        "Hierarchical approach: splits text using multiple separators (paragraph → sentence → word) iteratively. Balances context retention and chunk size consistency.\n",
        "\n",
        "Use Case Alignment:\n",
        "\n",
        "Character: Speed-focused tasks.\n",
        "\n",
        "Sentence/Token: NLP/model inputs.\n",
        "\n",
        "Recursive: Context-heavy retrieval (e.g., RAG)."
      ],
      "metadata": {
        "id": "U480XkGSo-ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    SentenceTransformersTokenTextSplitter,\n",
        "    TokenTextSplitter,\n",
        "    TextSplitter,\n",
        ")"
      ],
      "metadata": {
        "id": "7lMadRiezbsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CZNYD63HR1v"
      },
      "source": [
        "# Character Based Text Splitter\n",
        "\n",
        "Tries to split at \\n\\n (default separator)\n",
        "\n",
        "If no split found, tries \\n\n",
        "\n",
        "The text contains sections without natural split points (like paragraphs without line breaks), forcing the splitter to create larger chunks than requested.\n",
        "\n",
        "If the chunk is longer than the specified size, a Warning will be shown for each chunk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DacaV_3UHR1v",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 1.1 Character-based splitting default using \\n\\n\n",
        "print(\"\\n--- Character-based Splitting ---\")\n",
        "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "char_docs = char_splitter.split_documents(documents)\n",
        "create_vector_store(char_docs, \"chroma_db_char\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.2 Character-based splitting with space as the separator\n",
        "print(\"\\n--- Character-based Splitting with \\\" \\\"---\")\n",
        "char_splitter = CharacterTextSplitter(separator=\" \", chunk_size=1000, chunk_overlap=100)\n",
        "char_docs_space = char_splitter.split_documents(documents)\n",
        "create_vector_store(char_docs_space, \"chroma_db_char_space\")"
      ],
      "metadata": {
        "id": "nj7NsV_ipPiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Based Text Splitting\n",
        "\n",
        "Splits text into chunks based on sentences, ensuring chunks end at sentence boundaries.\n",
        "Ideal for maintaining semantic coherence within chunks."
      ],
      "metadata": {
        "id": "4MhSQtNzxe3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Sentence-based splitting\n",
        "print(\"\\n--- Sentence-based Splitting ---\")\n",
        "sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
        "sent_docs = sent_splitter.split_documents(documents)\n",
        "create_vector_store(sent_docs, \"chroma_db_sent\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fiLglBkayW4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token-based Splitting\n",
        "Splits text into chunks based on tokens (words or subwords), using tokenizers like GPT-2.\n",
        "Useful for transformer models with strict token limits."
      ],
      "metadata": {
        "id": "7LvjOciez8-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "# 3 Token-based splitting\n",
        "print(\"\\n--- Using Token-based Splitting ---\")\n",
        "token_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\n",
        "token_docs = token_splitter.split_documents(documents)\n",
        "create_vector_store(token_docs, \"chroma_db_token\")"
      ],
      "metadata": {
        "id": "PSQ3Zgvc0ScF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Recursive Character-based Splitting**\n",
        "Attempts to split text at natural boundaries (sentences, paragraphs) within character limit.\n",
        "\n",
        "Balances between maintaining coherence and adhering to character limits.\n"
      ],
      "metadata": {
        "id": "YBDWZQJj09OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Recursive Character-based splitting\n",
        "print(\"\\n--- Using Recursive Character-based Splitting ---\")\n",
        "rec_char_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100)\n",
        "rec_char_docs = rec_char_splitter.split_documents(documents)\n",
        "create_vector_store(rec_char_docs, \"chroma_db_rec_char\")"
      ],
      "metadata": {
        "id": "mOU9md6b1JnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_store(store_name, query):\n",
        "    # point at your existing persistent_directory\n",
        "    store_path = os.path.join(persistent_directory, store_name)\n",
        "    if os.path.exists(store_path):\n",
        "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
        "        db = Chroma(\n",
        "            persist_directory=store_path,\n",
        "            embedding_function=embeddings\n",
        "        )\n",
        "        retriever = db.as_retriever(\n",
        "            search_type=\"similarity_score_threshold\",\n",
        "            search_kwargs={\"k\": 1, \"score_threshold\": 0.5},\n",
        "        )\n",
        "        relevant_docs = retriever.invoke(query)\n",
        "        # Display the relevant results with metadata\n",
        "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
        "        for i, doc in enumerate(relevant_docs, 1):\n",
        "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "            if doc.metadata:\n",
        "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
        "    else:\n",
        "        print(f\"Vector store {store_name} does not exist.\")\n",
        "\n",
        "# Define the user's question\n",
        "query = \"Where is Oceania?\"\n",
        "\n",
        "# Query each vector store\n",
        "query_vector_store(\"chroma_db_char\", query)\n",
        "query_vector_store(\"chroma_db_char_space\", query)\n",
        "query_vector_store(\"chroma_db_sent\", query)\n",
        "query_vector_store(\"chroma_db_token\", query)\n",
        "query_vector_store(\"chroma_db_rec_char\", query)\n",
        "query_vector_store(\"chroma_db_custom\", query)"
      ],
      "metadata": {
        "id": "Xn1kby7M2Pie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup LLM**"
      ],
      "metadata": {
        "id": "CPQA7UL6IYIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Set up Google Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp-image-generation\")\n",
        "\n",
        "# Your question\n",
        "query = \"What is the name of the city or place where 1984 is set?\"\n",
        "\n",
        "# List all your split‐based stores\n",
        "stores = [\n",
        "    \"chroma_db_char\",\n",
        "    \"chroma_db_char_space\",\n",
        "    \"chroma_db_sent\",\n",
        "    \"chroma_db_token\",\n",
        "    \"chroma_db_rec_char\",\n",
        "    \"chroma_db_custom\",\n",
        "]\n",
        "\n",
        "for store_name in stores:\n",
        "    store_path = os.path.join(persistent_directory, store_name)\n",
        "    if not os.path.exists(store_path):\n",
        "        print(f\"Skipping missing store: {store_name}\")\n",
        "        continue\n",
        "\n",
        "    # Load the vector store with the same embeddings you used to create it\n",
        "    db = Chroma(\n",
        "        persist_directory=store_path,\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "\n",
        "    # Build a RetrievalQA chain for this store\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm,\n",
        "        retriever=db.as_retriever(\n",
        "            search_type=\"similarity_score_threshold\",\n",
        "            search_kwargs={\"k\": 3, \"score_threshold\": 0.5}\n",
        "        ),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    # Invoke the chain\n",
        "    result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "    # Print the output\n",
        "    print(f\"\\n=== Results using store: {store_name} ===\")\n",
        "    print(\"\\n--- Answer ---\")\n",
        "    print(result[\"result\"])\n",
        "\n",
        "    # Print the single “chunk used” (top‐ranked doc)\n",
        "    if result[\"source_documents\"]:\n",
        "        top = result[\"source_documents\"][0]\n",
        "        print(\"\\n--- Chunk Used ---\")\n",
        "        print(top.page_content)\n",
        "    else:\n",
        "        print(\"\\n--- Chunk Used ---\")\n",
        "        print(\"No chunk passed the threshold.\")\n"
      ],
      "metadata": {
        "id": "BUKJ_8Dh5wjg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}